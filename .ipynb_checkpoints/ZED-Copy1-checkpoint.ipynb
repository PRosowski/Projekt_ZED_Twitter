{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt ZED - analiza tweetów\n",
    "\n",
    "### Przemysław Rosowski, 110289\n",
    "\n",
    "Celem tego projektu jest przeanalizowanie danych zgromadzonych z portalu Twitter oraz klasyfikacja tweetów do klas: pozytywna, negatywna, neutralna.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Zaimportowane biblioteki\n",
    "\n",
    "Do wykonania projektu zostały wykorzystane poniższe biblioteki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dane\n",
    "\n",
    "Dane treningowe pobrane z pliku train.csv, a testowe w pliku test.csv. W celu sprawdzenia precyzji klasyfikatorów podzielono zbiór danych traningowych na dwa zbiory - jeden uczący i jeden treningowy.\n",
    "\n",
    "Kod, który odpowiada za generowanie wyników z danych testowych umieszczonych w pliku test.csv jest zawarty w pliku ZED-submission.ipynb.\n",
    "\n",
    "Jedynym właściwie większym problemem na tym poziomie jest pojawienie się Tweetów z zawartością: \"Not available\". Niestety są one przyporządkowane do różnych klas. Postowiono zostawić takie Tweety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>641435745627033600</td>\n",
       "      <td>positive</td>\n",
       "      <td>What's weird is Serena and Venus are a train r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>641489626184728580</td>\n",
       "      <td>positive</td>\n",
       "      <td>Been rewatching 3rd Rock from the Sun on Netfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>637305548279533568</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Bielsa_1899 @DiMarzio i think it's impossible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>638760832822734848</td>\n",
       "      <td>negative</td>\n",
       "      <td>Prolifers held a large rally in Petoskey on Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4657</th>\n",
       "      <td>640774685743955968</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@totallymorgan you mean no Teen Wolf tomorrow?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Id  Category  \\\n",
       "3802  641435745627033600  positive   \n",
       "2440  641489626184728580  positive   \n",
       "1002  637305548279533568  positive   \n",
       "3028  638760832822734848  negative   \n",
       "4657  640774685743955968   neutral   \n",
       "\n",
       "                                                  Tweet  \n",
       "3802  What's weird is Serena and Venus are a train r...  \n",
       "2440  Been rewatching 3rd Rock from the Sun on Netfl...  \n",
       "1002  @Bielsa_1899 @DiMarzio i think it's impossible...  \n",
       "3028  Prolifers held a large rally in Petoskey on Sa...  \n",
       "4657  @totallymorgan you mean no Teen Wolf tomorrow?...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\", sep=\",\")\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=32)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Przetworzenie tekstu.\n",
    "\n",
    "Jedną z ważniejszych rzeczy jest przetworzenie tekstu. Zdefiniowano listę emotikon, które wykorzystane zostaną do ich zamiany na odpowiednie słowa. Do usuwania zbędnych słow została wykorzystana biblioteka nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "happy = set([\n",
    "    u':-)', u':)', u';)', u':o)', u':]', u':3', u':c)', u':>', u'=]', u'8)', u'=)', u':}',\n",
    "    u':^)', u':-D', u':D', u'8-D', u'8D', u'x-D', u'xD', u'X-D', u'XD', u'=-D', u'=D',\n",
    "    u'=-3', u'=3', u':-))', u\":'-)\", u\":')\", u':*', u':^*', u'>:P', u':-P', u':P', u'X-P',\n",
    "    u'x-p', u'xp', u'XP', u':-p', u':p', u'=p', u':-b', u':b', u'>:)', u'>;)', u'>:-)',\n",
    "    u'<3', u'8-)'\n",
    "    ])\n",
    "\n",
    "sad = set([\n",
    "    u':L', u':-/', u'>:/', u':S', u'>:[', u':@', u':-(', u':[', u':-||', u'=L', u':<',\n",
    "    u':-[', u':-<', u'=\\\\', u'=/', u'>:(', u':(', u'>.<', u\":'-(\", u\":'(\", u':\\\\', u':-c',\n",
    "    u':c', u':{', u'>:\\\\', u';('\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej zdefiniowana jest funkcja tokenize odpowiedzialna za wstępną obróbkę tekstu. W pierwszej fazie rozdzialamy \n",
    "pojedynczego tweeta na słowa. Następnie w pętli wyszukiwane są zbędne słowa. \n",
    "\n",
    "Następnie dla każdego słowa:\n",
    "    wyszukiwane są emotikony i w zależności czy jest ona negatywna czy pozytywna zastępowane są odpowiednimi słowami,\n",
    "    wyszukiwane są zawarte w tewwtach linki i usuwane,\n",
    "    zawarte w tweecie odwołania do innych użytkowników są usuwane,\n",
    "    usuwane są słowa zawierające liczby.\n",
    "    \n",
    "Potem wykonywany jest proces stemmingu, ucinane są końcówki słow. W ten sposób zminiejszamy liczbę słów w wynikowym słowniku. \n",
    "Po całej operacji pojedyncze słowa są łączone do postaci pojedynczego tweeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize( raw_tweet ):\n",
    "    words = raw_tweet.lower().split()   \n",
    "    words = [w for w in words if not w in stopwords]\n",
    "    for one_sentence in words:    \n",
    "        if one_sentence in sad:\n",
    "            words[words.index(one_sentence)] = 'bad fuck sad'\n",
    "        elif one_sentence in happy:\n",
    "            words[words.index(one_sentence)] = 'happi great happy'\n",
    "        elif re.match(r'^https?:\\/\\/.*[\\r\\n]*',one_sentence):\n",
    "            words[words.index(one_sentence)] = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', one_sentence, flags=re.MULTILINE)\n",
    "        elif re.match(r'(?:@[\\w_]+)',one_sentence):\n",
    "            words[words.index(one_sentence)] = re.sub(r'(?:@[\\w_]+)', '', one_sentence)\n",
    "        elif re.match(r'\\d+', one_sentence):\n",
    "            words[words.index(one_sentence)]  = ''\n",
    "        else:\n",
    "            words[words.index(one_sentence)] = re.sub(r'[^\\w\\s]','',one_sentence)\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return( \" \".join( words ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reprezentacja danych\n",
    "\n",
    "Ostanim krokiem przed klasyfikacją tweetów jest ich transformacja do postaci numerycznej.\n",
    "Wykorzystane zostało podejście Bag of Words, który \"uczy się\" słow z ze wszystkich tweetów, \n",
    "a następnie modeluje każdego tweeta poprzez zliczanie powtórzeń słów.\n",
    "\n",
    "Jednym ze sposobów zaimplementowania Bag of Words jest wykorzystanie bibliotek scikit-learn.\n",
    "Zaincjowano obiekt \"CountVectorizer\" zdodatkowymi parametrami wyłączającymi pewne funkcjonalności ( preprocessing, \n",
    "tokenizowanie i usuwanie zędnych słów zostało wykonane wcześniej). Odrzucono słowa występujące w mniej niż 10 tweetach oraz\n",
    "słowa, które występują w ponad 80% dokumentów.\n",
    "\n",
    "\n",
    "Funkcja fit_transform najpierw słownik a później wytwarza wektory w postaci słowo-waga.\n",
    "Następnie transform najpierw transformuje dane do odpowiadającej postaci.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_tweets = []\n",
    "\n",
    "for index, data in train.iterrows():\n",
    "    clean_train_tweets.append(tokenize(data['Tweet']))\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000,min_df = 10,max_df = 0.80) \n",
    "    \n",
    " \n",
    "train_features = vectorizer.fit_transform(clean_train_tweets)\n",
    "train_features = train_features.toarray()\n",
    "\n",
    "clean_test_tweets = [] \n",
    "\n",
    "for index, data in test.iterrows():\n",
    "    clean_test_tweets.append(tokenize(data['Tweet']))\n",
    "\n",
    "\n",
    "test_features = vectorizer.transform(clean_test_tweets)\n",
    "test_features = test_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Klasyfikacja\n",
    "\n",
    "Do procesu klasyfikacji zostały użyte 3 klasyfikatory : Regresja Logistyczna, Linear Support Vector Classification oraz Random Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wynik klasyfikacji metodą LogisticRegression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.41      0.27      0.33       277\n",
      "    neutral       0.45      0.37      0.41       641\n",
      "   positive       0.62      0.77      0.69       873\n",
      "\n",
      "avg / total       0.53      0.55      0.53      1791\n",
      "\n",
      "Wynik klasyfikacji metodą SVC()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.39      0.36      0.37       277\n",
      "    neutral       0.46      0.38      0.42       641\n",
      "   positive       0.63      0.73      0.68       873\n",
      "\n",
      "avg / total       0.53      0.55      0.54      1791\n",
      "\n",
      "Wynik klasyfikacji metodą RandomTreeForest\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.43      0.20      0.28       277\n",
      "    neutral       0.47      0.42      0.44       641\n",
      "   positive       0.63      0.78      0.69       873\n",
      "\n",
      "avg / total       0.54      0.56      0.54      1791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_classifier = LogisticRegression(penalty='l1')\n",
    "log_classifier.fit(train_features, train[\"Category\"])\n",
    "predictionLog = log_classifier.predict(test_features)\n",
    "\n",
    "LSVC_classifier = svm.LinearSVC()\n",
    "LSVC_classifier.fit(train_features, train[\"Category\"])\n",
    "predictionLSVC = LSVC_classifier.predict(test_features)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "forest = forest.fit( train_features, train[\"Category\"] )\n",
    "forest = forest.predict(test_features)\n",
    "\n",
    "\n",
    "print(\"Wynik klasyfikacji metodą LogisticRegression\")\n",
    "print(classification_report(test['Category'], predictionLog))\n",
    "\n",
    "print(\"Wynik klasyfikacji metodą SVC()\")\n",
    "print(classification_report(test['Category'], predictionLSVC))\n",
    "\n",
    "print(\"Wynik klasyfikacji metodą RandomTreeForest\")\n",
    "print(classification_report(test['Category'], forest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Do ostatecznego wyznaczania kategorii tweetów został wybrany klasyfikator Randon Forest. \n",
    "\n",
    "Tweety ze względu na swój charakter są trudne do analizowania. Zastosowano najpopularniejsze metody przetwarzania tekstu. Przyniosły one umiarkowanie dobre rezultaty. Część tweetów zawierała wartości \"Not Available\", język tych tweetów nie był oficjalny ( sporo uproszczeń, skrótów ). To spowodowało, że trafność klasyfikacji nie jest do końca zadowalająca.\n",
    "\n",
    "Kod generujący submission.csv zawarty jest a pliku ZED_submission.jpynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
